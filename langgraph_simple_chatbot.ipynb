{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5834f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f66802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state - Simple version!\n",
    "class ChatState(TypedDict):\n",
    "    messages: list  # Just a regular list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01a442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM (using Ollama with llama3.2)\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",  # or \"llama3.2:1b\", \"mistral\", \"phi3\"\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_node(state: ChatState) -> ChatState:\n",
    "    \"\"\"Main chatbot node that calls the LLM\"\"\"\n",
    "    # Get all messages from state\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Call the LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Manually append the response to existing messages\n",
    "    updated_messages = messages + [response]\n",
    "    \n",
    "    # Return the complete updated message list\n",
    "    return {\"messages\": updated_messages}\n",
    "\n",
    "\n",
    "def create_chatbot():\n",
    "    \"\"\"Build the chatbot graph\"\"\"\n",
    "    # Create the graph\n",
    "    workflow = StateGraph(ChatState)\n",
    "    \n",
    "    # Add the chatbot node\n",
    "    workflow.add_node(\"chatbot\", chatbot_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"chatbot\")\n",
    "    \n",
    "    # Add edge to END\n",
    "    workflow.add_edge(\"chatbot\", END)\n",
    "    \n",
    "    # Compile with memory to maintain conversation history\n",
    "    memory = MemorySaver()\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fe3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    \"\"\"Interactive chat loop\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ü§ñ Simple Chatbot with LangGraph + Ollama\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Type 'quit', 'exit', or 'bye' to end the conversation\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Create the chatbot\n",
    "    app = create_chatbot()\n",
    "    \n",
    "    # Configuration for conversation thread\n",
    "    config = {\"configurable\": {\"thread_id\": \"user_session_1\"}}\n",
    "    \n",
    "    # System message to set the chatbot's behavior\n",
    "    conversation_history = [\n",
    "        SystemMessage(content=\"You are a helpful, friendly AI assistant. Keep your responses concise and engaging.\")\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        # Check for exit commands\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye', 'goodbye']:\n",
    "            print(\"\\nü§ñ Chatbot: Goodbye! Have a great day! üëã\")\n",
    "            break\n",
    "        \n",
    "        # Skip empty inputs\n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Add user message to conversation history\n",
    "        conversation_history.append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Create state with full conversation history\n",
    "        current_state = {\"messages\": conversation_history}\n",
    "        \n",
    "        # Get chatbot response\n",
    "        try:\n",
    "            result = app.invoke(current_state, config)\n",
    "            \n",
    "            # Update conversation history with the result\n",
    "            conversation_history = result[\"messages\"]\n",
    "            \n",
    "            # Display the AI's response (last message)\n",
    "            ai_message = conversation_history[-1]\n",
    "            print(f\"\\nü§ñ Chatbot: {ai_message.content}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "            print(\"Make sure Ollama is running and the model is downloaded.\")\n",
    "            print(\"Run: ollama pull llama3.2\")\n",
    "            break\n",
    "\n",
    "# Simple single question function\n",
    "def ask_question(question: str, model: str = \"llama3.2\"):\n",
    "    \"\"\"Ask a single question without conversation history\"\"\"\n",
    "    app = create_chatbot()\n",
    "    config = {\"configurable\": {\"thread_id\": \"single_qa\"}}\n",
    "    \n",
    "    state = {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "            HumanMessage(content=question)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    result = app.invoke(state, config)\n",
    "    return result[\"messages\"][-1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05986026",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
